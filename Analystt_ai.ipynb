{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Task1\n",
    "Items to scrape\n",
    "* Product URL\n",
    "* Product Name\n",
    "* Product Price\n",
    "* Rating\n",
    "* Number of reviews"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "463e319a88b07c72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define constants and global variables\n",
    "base_url = \"https://www.amazon.in\"\n",
    "url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\"\n",
    "number_of_pages = 20\n",
    "temp = 0  # Counter for tracking product IDs\n",
    "\n",
    "# Function to scrape Amazon products from a given URL\n",
    "def scrape_amazon_products(url):\n",
    "    global temp  # Use the global counter\n",
    "\n",
    "    # Define request headers\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Referer\": \"https://www.amazon.in/\",\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    data = []\n",
    "    print(\"1-=-==-==-=-==-=-==\")\n",
    "    # Extract product details from the current page\n",
    "    for product in soup.find_all(\"div\", {\"data-component-type\":\"s-search-result\"}):\n",
    "        print(\"2-=-==-==-=-==-=-==\")\n",
    "\n",
    "        product_url = base_url + product.find(\"a\", class_=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\").get(\"href\")\n",
    "        product_name = product.find(\"span\", class_=[\"a-size-medium a-color-base a-text-normal\",\"a-size-medium a-color-base a-text-bold\"]).text\n",
    "        product_price = float(product.find(\"span\", class_=\"a-price-whole\").text.replace(\",\", \"\"))\n",
    "        rating=0\n",
    "        try:\n",
    "            rating_span = product.find(\"span\", class_=[\"a-size-base\", \"a-icon-alt\",\"a-size-base puis-normal-weight-text\",\"a-size-base puis-bold-weight-text\"])\n",
    "            rating = float(rating_span.text.split()[0]) if rating_span else 0.0\n",
    "        except ValueError:\n",
    "            rating=0\n",
    "        number_of_review = product.find(\"span\", class_=[\"a-size-base s-underline-text\",\"a-size-base a-color-secondary totalRatingCount\"])\n",
    "        if isinstance(number_of_review, str) and 'K' in number_of_review:\n",
    "            try:\n",
    "                number_of_review = number_of_review.text.split()[0].replace(\",\", \"\").replace(\"(\", \"\").replace(\")\", \"\") if number_of_review else 0\n",
    "\n",
    "                number_of_review = int(float(number_of_review.replace('K', '').replace('+', '')) * 1000)\n",
    "            except ValueError:\n",
    "                number_of_review = 0  # Set to default if unable to convert\n",
    "        else:\n",
    "            try:\n",
    "                number_of_review = int(number_of_review)\n",
    "            except ValueError:\n",
    "                number_of_review = 0  # Set to default if unable to convert\n",
    "            except TypeError:\n",
    "                number_of_review = 0  # Set to default if unable to convert\n",
    "\n",
    "\n",
    "        # Append product details to the data list\n",
    "        f = [temp, product_url, product_name, product_price, rating, number_of_review]\n",
    "        data.append(f)\n",
    "        temp += 1\n",
    "\n",
    "    # Find the link to the next page, if available\n",
    "    pagination_container = soup.find('div', class_='s-pagination-container')\n",
    "    next_url_element = pagination_container.find('a', class_='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator')\n",
    "    next_url = base_url + next_url_element['href'] if next_url_element else None\n",
    "\n",
    "    return data, next_url\n",
    "\n",
    "# Main data collection loop\n",
    "datas = []\n",
    "\n",
    "for i in range(number_of_pages):\n",
    "    data, next_url = scrape_amazon_products(url)\n",
    "    if not next_url:\n",
    "        print(\"not next url \"+str(i))\n",
    "        break\n",
    "    url = next_url\n",
    "    datas.extend(data)\n",
    "\n",
    "# Create a pandas DataFrame from the collected data and save it to a CSV file\n",
    "df = pd.DataFrame(datas, columns=[\"Product ID\", \"Product URL\", \"Product Name\", \"Product Price\", \"Rating\", \"Review Count\"])\n",
    "df.to_csv(\"amazon_products.csv\", index=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task2\n",
    "With the Product URL received in the above case, hit each URL, and add below items:\n",
    "* Description\n",
    "* ASIN\n",
    "* Product Description\n",
    "* Manufacturer\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dad5eb364ac4c01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the previously scraped data from CSV\n",
    "df = pd.read_csv(\"amazon_products.csv\")\n",
    "data = []\n",
    "\n",
    "# Iterate through the DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    print(index)\n",
    "    url = row[\"Product URL\"]\n",
    "    product_id = row[\"Product ID\"]\n",
    "\n",
    "    # Define request headers\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Referer\": \"https://www.amazon.in/\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    try:\n",
    "\n",
    "        # Extract description and product description\n",
    "        description = soup.find(\"div\", id=\"feature-bullets\").text\n",
    "        prod_desc_element = soup.find(\"div\", id=\"productDescription\")\n",
    "        prod_desc = prod_desc_element.p.span.text if prod_desc_element else \"\"\n",
    "    \n",
    "        # Extract ASIN and Manufacturer information\n",
    "        prodiv = soup.find(\"div\", \"a-section feature detail-bullets-wrapper bucket\")\n",
    "        if prodiv is not None:\n",
    "            thead = prodiv.find_all(\"span\", \"a-list-item\")\n",
    "            asinno = \"\"\n",
    "            manufacturer = \"\"\n",
    "        \n",
    "            for i in range(len(thead)):\n",
    "                try:\n",
    "                    span_text = thead[i].find_all(\"span\")[0].text.split()[0]\n",
    "                    # print(thead[i].find_all(\"span\"))\n",
    "                    if span_text == \"ASIN\":\n",
    "                        asinno = thead[i].find_all(\"span\")[1].text\n",
    "                    elif span_text == \"Manufacturer\":\n",
    "                        manufacturer = thead[i].find_all(\"span\")[1].text\n",
    "                except IndexError:\n",
    "                    asinno=\"0\"\n",
    "                    manufacturer=\"0\"\n",
    "            data.append([product_id, description, prod_desc, asinno, manufacturer])\n",
    "    except AttributeError:\n",
    "        print(\"AttributeError\")\n",
    "        \n",
    "# Create a DataFrame from the collected data and merge it with the original DataFrame\n",
    "df1 = pd.DataFrame(data, columns=[\"Product ID\", \"Description\", \"Product Description\", \"ASIN\", \"Manufacturer\"])\n",
    "df = df.merge(df1, on=\"Product ID\")\n",
    "\n",
    "# Save the merged DataFrame back to CSV\n",
    "df.to_csv(\"amazon_products.csv\", index=False)\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ea4bae2d255b5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "95039ebd113cac69"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
